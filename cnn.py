# -*- coding: utf-8 -*-
"""cnn2.ipynb
@ Hugo Rodriguez
Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Idn3CUof4nC8ZN0naP5_Ae4h6GnnSv-w


"""

# # Mount drive to load images into memory
# from google.colab import drive
# drive.mount("/content/gdrive", force_remount=False)

import numpy as np
import os
import time
import sys
from matplotlib import pyplot as plt
from PIL import Image
import tensorflow as tf
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, BatchNormalization, InputLayer
from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import SGD, Adam
from tensorflow.keras.constraints import MaxNorm, MinMaxNorm
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report

# Load and standardize training images, load traing labels 
X = np.load() 
X = X / 255
y = np.load("/content/gdrive/My Drive/Colab Notebooks/Labels_32.npy")

X_train, X_val, y_train, y_val = train_test_split(X, to_categorical(y, 5), test_size=0.5, random_state=42)

print(X_train.shape, y_train.shape)
print(X_val.shape, y_val.shape)

def create_model():
  model = Sequential([
            InputLayer(input_shape=(200,200,3)),

            #VGG Block 1
            Conv2D(32, (3, 3), strides=(1,1), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            MaxPool2D((2, 2), strides=2),
            
            # VGG Block 2
            Conv2D(64, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            MaxPool2D((2, 2), strides=2),

            # VGG Block 3
            Conv2D(128, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            Conv2D(128, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            MaxPool2D((2, 2), strides=2),

            # VGG Block 4
            Conv2D(256, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            Conv2D(256, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            MaxPool2D((2, 2), strides=2),

            # VGG Block 5
            Conv2D(256, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            Conv2D(256, (3, 3), activation="relu", kernel_initializer='he_uniform',
                                          padding='same'),
            BatchNormalization(),
            MaxPool2D((2, 2), strides=2),
            
            #Fully Connected Layer
            Flatten(),
            Dense(2048, activation='relu', kernel_initializer='he_uniform', kernel_constraint=MaxNorm(5)),
            Dropout(.5),
            Dense(256, activation='relu', kernel_initializer='he_uniform', kernel_constraint=MaxNorm(5)),
            Dropout(.5),
            Dense(5, activation='softmax')


            
  ])
  return model

opt = SGD(lr=.01, momentum=.9)
callbacks = [
             EarlyStopping(restore_best_weights=True, monitor='val_loss', patience=20),
             ReduceLROnPlateau(monitor='val_loss', factor=.1, patience=10)
]
model = create_model()
model.summary()
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])

history = model.fit(
    X_train, y_train, 
    validation_data=(X_val, y_val), 
    epochs=1000, 
    batch_size=16, 
    callbacks=callbacks
)

# Plot Training Accuracy vs Iternation
plt.subplot(211)
plt.title('Classification Accuracy')
plt.plot(history.history['accuracy'], color='blue', label='train')
plt.plot(history.history['val_accuracy'], color='orange', label='validation')
plt.legend(loc="lower right")
plt.savefig('classification_accuracy.png')
plt.show()
plt.close()

# Plot Traing Loss vs Iteration
plt.subplot(211)
plt.title('Cross Entropy Loss')
plt.plot(history.history['loss'], color='blue', label='train')
plt.plot(history.history['val_loss'], color='orange', label='test')
plt.legend(loc="upper right")
plt.savefig('cross_entropy_loss.png')
plt.show()
plt.close()

# Test Model & Generate Confusion Matrix
y_pred = np.argmax(model.predict(X_val), 1)
cr = classification_report(np.argmax(y_val, 1), y_pred)
cf = tf.math.confusion_matrix(labels=np.argmax(y_val, 1), predictions=y_pred)
results = model.evaluate(X_val, y_val)

# Output Test Results and Confusion Matrix
print("Loss: " + str(results[0]) + ", " + "Accuracy: " + str(results[1]))
print(cr)
print(cf)